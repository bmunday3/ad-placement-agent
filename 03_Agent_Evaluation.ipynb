{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966879a5-16b8-4c98-91f9-f4aa2ed70e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the Databricks LangChain integration package\n",
    "# Install Unity Catalog AI integration packages with the Databricks extra\n",
    "\n",
    "%pip install --upgrade databricks-agents unitycatalog-ai[databricks] unitycatalog-langchain[databricks] databricks-langchain databricks-vectorsearch==0.56 langchain==0.3.20 langgraph==0.3.4 pydantic==2.11.7 mlflow[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ded1da-88d0-45c8-b716-68d60e1d7729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = 'media_advertising'\n",
    "SCHEMA = 'contextual_advertising'\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc0d72e-7e17-4b00-9665-56191ee3991f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluation Process\n",
    "\n",
    "1. Log RAG Agent with MLflow and Register & Deploy Agent\n",
    "2. Create a custom metric that we can leverage in the evaluation process\n",
    "3. Create a synthetic dataset of possible requests using generate_eval_df()\n",
    "4. Generate responses from the existing agent based on the synthetic dataset\n",
    "5. Pass the request-response pairs to Agent Evaluation LLM as a judge using built-in-metrics and the custom metric defined\n",
    "6. Spin up human evaluations and have \"experts\" evaluate\n",
    "7. Walk through an example human review via the review app, and show how it can be synced to a dataset.\n",
    "\n",
    "\n",
    "**Note**: We're operating with an already deployed endpoint, there's code at the end that shows how to deploy an endpoint which would need to occur prior to executing the code that leverages the endpoint in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde726f3-638d-4d3a-ba1a-19c0b3ec8e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Log RAG Agent with MLflow\n",
    "First, we'll log our RAG Agent we defined in [Notebook #2](./02_Agent_Definition) to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8dfb693-a66e-447a-84eb-bf903e761182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model to MLflow\n",
    "import os\n",
    "import mlflow\n",
    "print(f'mlflow version: {mlflow.__version__}')\n",
    "\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"When could I insert a commercial for a light hearted basketball-themed comedy movie we want to promote for next summer?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print(os.path.join(\n",
    "            os.getcwd(),\n",
    "            '02_Agent_Definition',\n",
    "        ))\n",
    "mlflow.langchain.autolog()\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(\n",
    "            os.getcwd(),\n",
    "            '02_Agent_Definition',\n",
    "        ),\n",
    "        pip_requirements=[\n",
    "            \"langgraph==0.3.4\",\n",
    "            \"pydantic==2.11.7\",\n",
    "            \"databricks-vectorsearch==0.56\",\n",
    "            \"langchain==0.3.20\",\n",
    "            \"databricks-langchain\", # used for the retriever tool\n",
    "        ],\n",
    "        model_config=\"config.yml\",\n",
    "        artifact_path='agent',\n",
    "        input_example=input_example,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2033c129-3459-4f9f-a984-7e6edbbae446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Register & Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f31ab1e-bc68-46cf-8fbd-a58995ad0978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register Model to Unity Catalog\n",
    "First, we'll register our model to Unity Catalog in the same catalog and schema we've been using to organize our scripts in volumes and tables for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0ac92d-7ea3-444b-a151-0b0661b5a058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Use the workspace client to retrieve information about the current user\n",
    "w = WorkspaceClient()\n",
    "user_email = w.current_user.me().display_name\n",
    "username = user_email.split(\"@\")[0]\n",
    "\n",
    "# Catalog and schema have been automatically created\n",
    "catalog_name = CATALOG\n",
    "schema_name = SCHEMA\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "model_name = \"movie_scripts_chatbot_agent\" # Change to a different model name if desired\n",
    "UC_MODEL_NAME = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656799eb-a6cf-41cd-9ffd-ff40d4744906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy Agent\n",
    "Next, we'll deploy this RAG Agent with Agent Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72540377-ead6-4ce8-bdba-e76c06784917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start from this cell if model is already registered\n",
    "import mlflow\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "catalog_name = CATALOG\n",
    "schema_name = SCHEMA\n",
    "model_name = \"movie_scripts_chatbot_agent\" # Change to a different model name if desired\n",
    "UC_MODEL_NAME = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "uc_registered_model_info = client.get_model_version(name=UC_MODEL_NAME, version=uc_registered_model_info.version)\n",
    "print(uc_registered_model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ded738a-3984-4f46-8c8f-fc77bd314f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Deploy the model to the review app and a model serving endpoint\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"playground\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488d82ad-5ee9-4638-a243-88be978c6408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate Synthetic Data\n",
    "\n",
    "First we'll grab some scripts from our scripts database, and then create a custom synthetic dataset based on our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75bb7c0-6574-40ad-bf8f-06c80a47c319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "\n",
    "catalog_name = CATALOG\n",
    "schema_name = SCHEMA\n",
    "\n",
    "# Load in scripts and get a random sample to generate mock examples\n",
    "volume_path = f'/Volumes/{catalog_name}/{schema_name}/scripts'\n",
    "movie_scripts_df = spark.read.format('delta').load(volume_path)\n",
    "random_movies_scripts = movie_scripts_df.sample(withReplacement=False, fraction=0.03, seed=42) # Raise or lower fraction to increase/decrease sample size\n",
    "uri_constant = f'{catalog_name}.{schema_name}.raw_movie_scripts'\n",
    "sample_df = random_movies_scripts.select(\n",
    "    col(\"script\").alias(\"content\"),\n",
    "    concat(lit(uri_constant), lit(\"_\"), col(\"unique_movie_id\")).alias(\"doc_uri\")\n",
    ")\n",
    "display(sample_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe15529-e286-49ce-9210-20887b60d7d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# generate_evals_df\n",
    "\n",
    "**Purpose**: Generate a synthetic dataset of requests to use to evaluate our agent. \n",
    "\n",
    "- `question_guidelines`: Gives instructions how to generate the required input data to bootstrap an eval dataset\n",
    "- `agent_description`: Gives an overview of the purpose of the agent to help guide the questions\n",
    "- `docs`: Documents that are used for the sample request generation. \n",
    "- `num_evals`: The number of observations to create (total, not per document)\n",
    "\n",
    "We're grabbing 50 random scripts as the means to generate the data, and are asking the model to create generic user requests (rather than movie specific). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e3fbbf-af4e-4adf-b7a2-ba54625c7965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the synthetic data generation\n",
    "\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- An account executive who is responsible for contextual ad placement within shows\n",
    "- An enterprise executive who is responsible for the P&L and wants to optimize ad placement in shows\n",
    "\n",
    "# Example questions\n",
    "- When could I insert a commercial for a light hearted comedy movie we want to promote for next summer?\n",
    "- When could I insert a commercial for a boys and girls club non-profit ad campaign?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Question should be succinct with the goal of optimizing the relevance of advertising within a script.\n",
    "- The question should be generic, use the documents as a generalized framework to ask questions about movies.\n",
    "- NEVER reference specific scenes or characters. The full application will be asking questions across multiple scripts at once, not a specific show.\n",
    "\"\"\"\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a RAG chatbot that aims to recommend the optimal placement for advertising within scripts. The scripts are movies, but they are still intending to air commercials even though that is traditionally associated with TV. The Agent has access to a movie metadata and genre, and its task is to answer the user's questions by retrieving the relevant script chunks from the corpus and synthesizing a helpful, accurate response of where it makes sense to insert the ad placement. End users will be using this agent across many scripts at once, so questions will be generic across the full script database, rather than about specific shows.\n",
    "\"\"\"\n",
    "\n",
    "eval_df = generate_evals_df(docs=sample_df, num_evals=50, agent_description=agent_description, question_guidelines=question_guidelines)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d641f319-96d8-4d36-9a27-a9310d9fe4e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key outputs of the synthetic data generated**\n",
    "\n",
    "- `request`: This is the mock user request that we will use with our agent\n",
    "- `expected_retrieved_content`: The context the evaluation has been sourced from. This _can_ be useful, but it's restricted to a single document, so might not be as useful depending on the application. We will not be using that for this exercise.\n",
    "- `expected_facts`: Expected facts that should be returned in the response. Similar to `expected_retreived_content`, this _can_ be useful, but it's restricted to a single document, so might not be as useful depending on the application. We will not be using that for this exercise\n",
    "- `source_id`: The document used to produce the observation\n",
    "\n",
    "More detail can be found here: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/synthesize-evaluation-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12723e16-51f0-4f1f-b8d3-5173f7217a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate evaluations\n",
    "\n",
    "`evaluate` is the backbone of the evaluation process, and you can generate evaluations in multiple ways. In this case, we're going to be pre-computing request-response pairs, and pass those to the LLM as a judge. As an alternative, you can pass requests only and have your agent compute responses on the fly, which are then evaluated. This approach allows a bit more flexibility where you can customize what is showing up in the request and response pairs, as well as pull in observations that are potentially happening outside of a Databricks-built Agent/Endpoint. In this case, we're just going to pass the full input/output returned by the model to be evaluated.\n",
    "\n",
    "Additionally we've provided some guidelines for the judge to leverage when evaluating each observation.\n",
    "\n",
    "So in the cell below, we'll be calling our endpoint with requests we generated in the synthetic data generation process, and computing responses from our agent. It should take roughly ~20 minutes to generate 50 examples on Serverless compute.\n",
    "\n",
    "For more background on passing in evaluations, see this documentation: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/evaluation-schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad5bc3e-de73-4ff2-ae09-9b383125183a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "from databricks.agents import datasets\n",
    "# Leverage the created mock dataset for evaluation\n",
    "\n",
    "# Add in a guideline about genre etc...?\n",
    "\n",
    "agent_endpoint = 'agents_media_advertising-contextual_advertising-movie_scripts_c' \n",
    "\n",
    "guidelines = ['The retrieved content from the script database must be contextually relevant to the user request.',\n",
    "              'The retreived content must be relevant to making ad placement decisions.']\n",
    "\n",
    "request_list = [cont['messages'][0]['content'] for cont in eval_df[\"inputs\"].tolist()]\n",
    "client = get_deploy_client()\n",
    "endpoint = agent_endpoint\n",
    "agent_output_list = []\n",
    "for request in request_list: # Call our endpoint and compile the request response pairs\n",
    "  output = client.predict(endpoint=endpoint, inputs={\"messages\": [{\"role\": \"user\", \"content\": request}]})\n",
    "  agent_output_list.append({\"request\": request, \"response\": output['choices'][0]['message']['content']})\n",
    "\n",
    "# Create evaluation dataset \n",
    "# Could also add in the expected retreived content and expected facts, but per the commentary above they don't fit well because they are generated from one document.\n",
    "\n",
    "evals = [{\n",
    "  \"inputs\": {'query': output['request']},\n",
    "  \"response\": output['response'], \n",
    "  \"guidelines\": guidelines\n",
    "} for output in agent_output_list]\n",
    "\n",
    "# Below is the dataset that is used for the Review App\n",
    "\n",
    "uc_eval_dataset = f'{catalog_name}.{schema_name}.review_app_dataset'\n",
    "datasets.delete_dataset(uc_eval_dataset) # Uncomment if interested in recreating the dataset\n",
    "dataset = datasets.create_dataset(uc_eval_dataset) # Uncomment out after initial creation\n",
    "evals_app = [{\n",
    "  \"request\": {'messages': [{\"role\": \"user\", \"content\": output['request']}]},\n",
    "  \"guidelines\": guidelines\n",
    "} for output in agent_output_list[10:15]] # Pick specific observations if interested - this is a random subset of the list\n",
    "dataset.merge_records(evals_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795484f7-5686-4fbe-8300-70d1c2796fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Execute Evaluation\n",
    "\n",
    "We're passing in the dataset we generated of request-response pairs, and including the custom metric we created at the top of the notebook (`script_fit_custom_metric`), as well as some of the built-in metrics that are useful for us in thie context. \n",
    "\n",
    "When the execution job completes, click on the link generates to view the results in the Tracing tab, and dig into any problematic observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371b3176-f81e-4959-b4ae-13721e8b1d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import (\n",
    "    Guidelines,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "with mlflow.start_run(run_name=\"Movie-Eval-LLM\"):\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=evals,\n",
    "        scorers=[\n",
    "            RelevanceToQuery(),\n",
    "            Safety(),\n",
    "            # You can have any number of guidelines.\n",
    "            Guidelines(\n",
    "                name=\"Script Relevance\",\n",
    "                guidelines= \"\"\"\n",
    "                    Your job is to assess whether the returned content is a good fit for the request.\n",
    "                    The agent you are evaluating is retreiving data from movie scripts and returning relevant items based on the users request.\n",
    "                    You need to determine whether the the retrieved content is relevant for the users needs.\n",
    "                    Users use this retrieved content to compare their request to prior scripts and as a means to allocate advertising in relevant contexts.\n",
    "                    Below are the details for different scores:\n",
    "                        1=The retreived content is not relevant AND could not be used for ad placement decisions\n",
    "                        2=The retreived content is not relevant OR could not be used for ad placement decisions\n",
    "                        3=The retreived content is relevant and could be used for some ad placement decisions\n",
    "                        4=The retreived content is relevant and could be used for most ad placement decisions\n",
    "                        5=The retreived content is relevant and could be used for all ad placement decisions\"\"\",\n",
    "            ),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb78d3a-f367-40fe-a9fb-7f560e1f5296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Human Evaluation\n",
    "\n",
    "Now we've got a robust LLM-as-a-Judge process, but for some problems (particularly high-value or risk, or problems that require significant contextual and subject matter understanding), human evaluation is necessary to feel confident in the solution.\n",
    "\n",
    "The **Review App** comes by default when you deploy a model. It can be used for custom labeling sessions, or just interacting directly with the agent. In this case, it is just enabled for free-form conversation with the agent.\n",
    "\n",
    "The Review App can be found at this location: https://e2-demo-west.cloud.databricks.com/ml/review-v2/chat?endpoint=agents_media_advertising-contextual_advertising-movie_scripts_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "949756c1-817d-49a0-a5ef-6c96813e76a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# END OF DEMO in E2-DEMO-WEST-WS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab1c5a9-679a-48c0-af66-b9d920e0189c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# The Code Below will not work in this Environment\n",
    "\n",
    "Due to lack of permissions. But it will work in other environments (including e2-field-demo-west). Taking this repo and leveraging in other more permissive environments will enable the functionality described below (labeling sessions for SMEs)\n",
    "\n",
    "More detail can be found at this link: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/review-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c367eb8-2523-4389-87c7-ceb449b3ccd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import mlflow\n",
    "from databricks.agents import review_app\n",
    "\n",
    "catalog_name = CATALOG\n",
    "schema_name = SCHEMA\n",
    "model_name = \"movie_scripts_chatbot_agent\" # Change to a different model name if desired\n",
    "UC_MODEL_NAME = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "uc_model_version = uc_registered_model_info.version # Update to a different version if the endpoint iterates\n",
    "user_list = [\"INSERT USERS\"] # Update with users that need permissions\n",
    "\n",
    "# Set a list of users to review the app\n",
    "# Note that <user_list> can specify individual users or groups.\n",
    "agents.set_permissions(model_name=UC_MODEL_NAME, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031200ea-06e4-4d65-a7fd-914b1b59ba24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents import datasets, review_app\n",
    "\n",
    "# register and create review app dataset\n",
    "\n",
    "exp_id = 'b822d31cbc8e4124913eabd45ad580bf' # Get experiment ID associated with deployed model\n",
    "run_id = 'a4e2e4d6b4004be390ffddfe44b6825c' # Get run ID associated with \n",
    "script_app = review_app.get_review_app(exp_id) #Experiment ID for the endpoint\n",
    "script_app = script_app.add_agent(\n",
    "  agent_name=\"script_agent\",\n",
    "  model_serving_endpoint=agent_endpoint,\n",
    "  overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2afd9d2f-dc19-4536-b84c-7c36474f3493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Labeling Sessions\n",
    "\n",
    "Below is a custom labeling session we're asking our subject matter experts to go through.\n",
    "\n",
    "`create_label_schema` defines the label details - in this case we've defined two different metrics, one is an **expectation**, which gets integrated into guidelines for the LLM as a Judge in the future, and the other as **feedback** which provides detail into the quality of the output for evaluation purposes. Our dataset contains 5 observations, and we're asking our reviewers to provide feedback on both of these metrics for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cf9438-9f97-4c69-88fa-6a90be76bbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create label schemas for the review app including instructions\n",
    "# Customize these as needed\n",
    "\n",
    "traces = mlflow.search_traces(run_id=run_id) # Need to get run_id\n",
    "quality_exp_label = script_app.create_label_schema(name='quality_expectation',\n",
    "                                        type='expectation',\n",
    "                                          title=\"Movie Quality Appropriateness for Ads\",\n",
    "                                          input=review_app.label_schemas.InputText(),\n",
    "                                          instruction=\"Evaluate whether the quality of the movie meets the requirement for the ad placement\",\n",
    "                                          enable_comment=True,\n",
    "                                          overwrite=True)\n",
    "\n",
    "quality_label = script_app.create_label_schema(name='quality_feedback',\n",
    "                                        type='feedback',\n",
    "                                          title=\"Quality of the retrieved content for Ad Placement\",\n",
    "                                          input=review_app.label_schemas.InputNumeric(min_value= 1.0, max_value=5.0),\n",
    "                                          instruction=\"\"\"Determine the quality of the retrieved content for ad placement decisions.\n",
    "                                          1=The retreived content is not relevant AND could not be used for ad placement decisions\n",
    "                                          2=The retreived content is not relevant OR could not be used for ad placement decisions\n",
    "                                          3=The retreived content is relevant and could be used for some ad placement decisions\n",
    "                                          4=The retreived content is relevant and could be used for most ad placement decisions\n",
    "                                          5=The retreived content is relevant and could be used for all ad placement decisions\"\"\",\n",
    "                                          enable_comment=True,\n",
    "                                          overwrite=True)\n",
    "\n",
    "# Create a labeling session to evaluate that specific dataset generated\n",
    "\n",
    "script_labeling_session = script_app.create_labeling_session(\n",
    "  name=\"script_demo_session\",\n",
    "  agent='script_agent',\n",
    "  assigned_users = [\"INSERT USERS\"],\n",
    "  label_schemas = ['quality_expectation', 'quality_feedback'] # Change to names of any metrics created\n",
    ")\n",
    "# Add the records from the dataset to the labeling session.\n",
    "script_labeling_session.add_dataset(uc_eval_dataset)\n",
    "# NOTE: This copies the traces into this labeling session so that labels do not modify the original traces.\n",
    "#script_labeling_session.add_traces(traces)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e5b4143-5b94-4202-810f-561b54f1d46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The dataset below is what is being updated by the user feedback. The guidelines in there include generic guideliens on each observation, as well as some feedback specific to the observation. In the second example, specific feedback not to return violent UFC or boxing movies.\n",
    "\n",
    "Now let's launch the review app and review an example, then reload the dataset to see it updated - see the link produced after running the cell above\n",
    "\n",
    "Note that this dataset is a different paradigm than the request-response approach we went through before, in this case just providing a request, and the response is generated in real time (which we will see in the UI!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30282a44-83bc-4681-90c0-49b6a28d4510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents import datasets, review_app\n",
    "\n",
    "script_app = review_app.get_review_app()\n",
    "script_labeling_session = script_app.get_labeling_sessions()[0]\n",
    "script_labeling_session.sync_expectations(uc_eval_dataset)\n",
    "display(spark.read.table(uc_eval_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc6f9914-ec0e-4358-b6fc-dde7811eb00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The cell below shows some example output generated by our reviewers in the form of feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f4f4fb-f061-4a54-a677-fb424c12dfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Check the progress of reviews\n",
    "\n",
    "for row in mlflow.search_traces(run_id=run_id).to_dict(orient='records'):\n",
    "  print(f'{row[\"request\"]}: {row[\"assessments\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6e579f-431e-439b-91d2-e51561bb5ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from IPython.display import Image\n",
    "\n",
    "image_name = \"dogfood.jpeg\" #\"catfood.jpg\"\n",
    "display(Image(image_name))\n",
    "\n",
    "w = WorkspaceClient()\n",
    "client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "with open(image_name, \"rb\") as image_file:\n",
    "  base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_data_uri = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"databricks-llama-4-maverick\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Describe this advertisement concisely in one sentence with a description of the brand and what is happening in the image\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": image_data_uri\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95764945-9586-4054-bbc9-71c83bbe9483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import SpanType\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "agent_input = f\"Where should I place the following advertisement: {response.choices[0].message.content}\"\n",
    "agent_endpoint = 'agents_movie_scripts-ad_placement_agent-movie_scripts_chatbot_a' \n",
    "\n",
    "@mlflow.trace(span_type=SpanType.LLM)\n",
    "def invoke(ag_input):\n",
    "  agent_client = get_deploy_client()\n",
    "  span = mlflow.get_current_active_span()\n",
    "  # Set the attribute to the span\n",
    "  span.set_attributes({\"model\": agent_endpoint})\n",
    "  output = agent_client.predict(endpoint=agent_endpoint, inputs={\"messages\": [{\"role\": \"user\", \"content\": ag_input}]})\n",
    "\n",
    "  return output['choices'][0]['message']['content']\n",
    "  #payload = {\"request\": request, \"response\": output['choices'][0]['message']['content']}\n",
    "model_output = invoke(agent_input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fb7bc4-a253-4164-a8a9-ff78c077c2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(model_output)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/wesley.pasfield@databricks.com/AandE_workshop_April_2025/env2.yaml",
    "dependencies": [
     "databricks-agents>=0.16.0",
     "mlflow>=2.20.2",
     "langgraph=0.3.0",
     "unitycatalog-ai[databricks]",
     "unitycatalog-langchain[databricks]",
     "langchain_databricks"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Agent_Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
