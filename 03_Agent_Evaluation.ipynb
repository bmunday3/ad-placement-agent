{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966879a5-16b8-4c98-91f9-f4aa2ed70e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the Databricks LangChain integration package\n",
    "# Install Unity Catalog AI integration packages with the Databricks extra\n",
    "\n",
    "%pip install --upgrade databricks-agents unitycatalog-ai[databricks] unitycatalog-langchain[databricks] databricks-langchain databricks-vectorsearch==0.56 langchain==0.3.20 langgraph==0.3.4 pydantic==2.11.7 mlflow[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ded1da-88d0-45c8-b716-68d60e1d7729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = 'media_advertising'\n",
    "SCHEMA = 'contextual_advertising'\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc0d72e-7e17-4b00-9665-56191ee3991f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Evaluation Process\n",
    " \n",
    "1. Create a synthetic dataset of possible requests using generate_eval_df()\n",
    "2. Create a custom metric that we can leverage in the evaluation process\n",
    "3. Pass the requests generated Agent Evaluation LLM as a judge using built-in-metrics and the custom metric defined\n",
    "4. Spin up human evaluations and have \"experts\" evaluate\n",
    "5. Walk through an example human review via the review app, and show how it can be synced to a dataset. (not included in this notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488d82ad-5ee9-4638-a243-88be978c6408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate Synthetic Data\n",
    "\n",
    "First we'll grab some scripts from our scripts database, and then create a custom synthetic dataset based on our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75bb7c0-6574-40ad-bf8f-06c80a47c319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "\n",
    "catalog_name = CATALOG\n",
    "schema_name = SCHEMA\n",
    "\n",
    "# Load in scripts and get a random sample to generate mock examples\n",
    "volume_path = f'/Volumes/{catalog_name}/{schema_name}/scripts'\n",
    "movie_scripts_df = spark.read.format('delta').load(volume_path)\n",
    "random_movies_scripts = movie_scripts_df.sample(withReplacement=False, fraction=0.03, seed=42) # Raise or lower fraction to increase/decrease sample size\n",
    "uri_constant = f'{catalog_name}.{schema_name}.raw_movie_scripts'\n",
    "sample_df = random_movies_scripts.select(\n",
    "    col(\"script\").alias(\"content\"),\n",
    "    concat(lit(uri_constant), lit(\"_\"), col(\"unique_movie_id\")).alias(\"doc_uri\")\n",
    ")\n",
    "display(sample_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe15529-e286-49ce-9210-20887b60d7d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# generate_evals_df\n",
    "\n",
    "**Purpose**: Generate a synthetic dataset of requests to use to evaluate our agent. \n",
    "\n",
    "- `question_guidelines`: Gives instructions how to generate the required input data to bootstrap an eval dataset\n",
    "- `agent_description`: Gives an overview of the purpose of the agent to help guide the questions\n",
    "- `docs`: Documents that are used for the sample request generation. \n",
    "- `num_evals`: The number of observations to create (total, not per document)\n",
    "\n",
    "We're grabbing 50 random scripts as the means to generate the data, and are asking the model to create generic user requests (rather than movie specific). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e3fbbf-af4e-4adf-b7a2-ba54625c7965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the synthetic data generation\n",
    "\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- An account executive who is responsible for contextual ad placement within shows\n",
    "- An enterprise executive who is responsible for the P&L and wants to optimize ad placement in shows\n",
    "\n",
    "# Example questions\n",
    "- When could I insert a commercial for a light hearted comedy movie we want to promote for next summer?\n",
    "- When could I insert a commercial for a boys and girls club non-profit ad campaign?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Question should be succinct with the goal of optimizing the relevance of advertising within a script.\n",
    "- The question should be generic, use the documents as a generalized framework to ask questions about movies.\n",
    "- NEVER reference specific scenes or characters. The full application will be asking questions across multiple scripts at once, not a specific show.\n",
    "\"\"\"\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a RAG chatbot that aims to recommend the optimal placement for advertising within scripts. The scripts are movies, but they are still intending to air commercials even though that is traditionally associated with TV. The Agent has access to a movie metadata and genre, and its task is to answer the user's questions by retrieving the relevant script chunks from the corpus and synthesizing a helpful, accurate response of where it makes sense to insert the ad placement. End users will be using this agent across many scripts at once, so questions will be generic across the full script database, rather than about specific shows.\n",
    "\"\"\"\n",
    "\n",
    "eval_df = generate_evals_df(docs=sample_df, num_evals=50, agent_description=agent_description, question_guidelines=question_guidelines)\n",
    "eval_df['inputs'] = [{'input': row['messages']} for row in eval_df['inputs']] # For Responses Agent\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d641f319-96d8-4d36-9a27-a9310d9fe4e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key outputs of the synthetic data generated**\n",
    "\n",
    "- `request`: This is the mock user request that we will use with our agent\n",
    "- `expected_retrieved_content`: The context the evaluation has been sourced from. This _can_ be useful, but it's restricted to a single document, so might not be as useful depending on the application. We will not be using that for this exercise.\n",
    "- `expected_facts`: Expected facts that should be returned in the response. Similar to `expected_retreived_content`, this _can_ be useful, but it's restricted to a single document, so might not be as useful depending on the application. We will not be using that for this exercise\n",
    "- `source_id`: The document used to produce the observation\n",
    "\n",
    "More detail can be found here: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/synthesize-evaluation-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12723e16-51f0-4f1f-b8d3-5173f7217a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate evaluations\n",
    "\n",
    "`evaluate` is the backbone of the evaluation process, and you can generate evaluations in multiple ways. In this case, we're going to be pre-computing request-response pairs, and pass those to the LLM as a judge. As an alternative, you can pass requests only and have your agent compute responses on the fly, which are then evaluated. This approach allows a bit more flexibility where you can customize what is showing up in the request and response pairs, as well as pull in observations that are potentially happening outside of a Databricks-built Agent/Endpoint. In this case, we're just going to pass the full input/output returned by the model to be evaluated.\n",
    "\n",
    "Additionally we've provided some guidelines for the judge to leverage when evaluating each observation.\n",
    "\n",
    "So in the cell below, we'll be calling our endpoint with requests we generated in the synthetic data generation process, and computing responses from our agent. It should take roughly ~20 minutes to generate 50 examples on Serverless compute.\n",
    "\n",
    "For more background on passing in evaluations, see this documentation: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/evaluation-schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad5bc3e-de73-4ff2-ae09-9b383125183a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "from databricks.agents import datasets\n",
    "# Leverage the created mock dataset for evaluation\n",
    "\n",
    "# Add in a guideline about genre etc...?\n",
    "\n",
    "agent_endpoint = 'agents_media_advertising-contextual_advertising-movie_scripts_c' \n",
    "\n",
    "guidelines = ['The retrieved content from the script database must be contextually relevant to the user request.',\n",
    "              'The retreived content must be relevant to making ad placement decisions.']\n",
    "\n",
    "request_list = [cont['messages'][0]['content'] for cont in eval_df[\"inputs\"].tolist()]\n",
    "client = get_deploy_client()\n",
    "endpoint = agent_endpoint\n",
    "agent_output_list = []\n",
    "for request in request_list: # Call our endpoint and compile the request response pairs\n",
    "  output = client.predict(endpoint=endpoint, inputs={\"messages\": [{\"role\": \"user\", \"content\": request}]})\n",
    "  agent_output_list.append({\"request\": request, \"response\": output['choices'][0]['message']['content']})\n",
    "\n",
    "# Create evaluation dataset \n",
    "# Could also add in the expected retreived content and expected facts, but per the commentary above they don't fit well because they are generated from one document.\n",
    "\n",
    "evals = [{\n",
    "  \"inputs\": {'query': output['request']},\n",
    "  \"response\": output['response'], \n",
    "  \"guidelines\": guidelines\n",
    "} for output in agent_output_list]\n",
    "\n",
    "# Below is the dataset that is used for the Review App\n",
    "\n",
    "uc_eval_dataset = f'{catalog_name}.{schema_name}.review_app_dataset'\n",
    "datasets.delete_dataset(uc_eval_dataset) # Uncomment if interested in recreating the dataset\n",
    "dataset = datasets.create_dataset(uc_eval_dataset) # Uncomment out after initial creation\n",
    "evals_app = [{\n",
    "  \"request\": {'messages': [{\"role\": \"user\", \"content\": output['request']}]},\n",
    "  \"guidelines\": guidelines\n",
    "} for output in agent_output_list[10:15]] # Pick specific observations if interested - this is a random subset of the list\n",
    "dataset.merge_records(evals_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795484f7-5686-4fbe-8300-70d1c2796fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Execute Evaluation\n",
    "\n",
    "We're passing in the dataset we generated of request-response pairs, and including the custom metric we created at the top of the notebook (`script_fit_custom_metric`), as well as some of the built-in metrics that are useful for us in thie context. \n",
    "\n",
    "When the execution job completes, click on the link generates to view the results in the Tracing tab, and dig into any problematic observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371b3176-f81e-4959-b4ae-13721e8b1d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType, Feedback\n",
    "from typing import Any, Optional\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Define the prompts for the Judge LLM.\n",
    "judge_system_prompt = \"\"\"\n",
    "You are an impartial AI assistant responsible for evaluating the quality of a response generated by another AI model.\n",
    "Your evaluation should be based on the original user query and the AI's response.\n",
    "The context of the conversations is an user looking to find the most relevant script for their advertising scenario\n",
    "Provide a quality score as an integer from 1 to 5 (1=Poor, 2=Fair, 3=Good, 4=Very Good, 5=Excellent) that reflects the scene relevance.\n",
    "A 5 should reflect a perfect fit, a 1 should reflect a very weak or completely incorrect fit.\n",
    "Also, provide a brief rationale for your score in under 200 tokens. Be a very harsh critic and give out 5s sparingly. \n",
    "ONLY evaluate the final answer given, do not evaluate the options provided from the vector search tool call process\n",
    "\n",
    "Your output MUST ONLY be a single valid JSON object with two keys: \"score\" (an integer) and \"rationale\" (a string).\n",
    "The format must conform to this format\n",
    "Example:\n",
    "{\"score\": 4, \n",
    " \"rationale\": \"The scene returned fit the user request well but not perfectly and the format was correct.\"}\n",
    "\"\"\"\n",
    "judge_user_prompt = \"\"\"\n",
    "Please evaluate the AI's Response below based on the Original User Query.\n",
    "\n",
    "Original User Query:\n",
    "```{user_query}```\n",
    "\n",
    "AI's Response:\n",
    "```{llm_response_from_app}```\n",
    "\n",
    "Provide your evaluation strictly as a JSON object with \"score\" and \"rationale\" keys.\n",
    "\"\"\"\n",
    "\n",
    "@scorer\n",
    "def script_fit_custom_metric(inputs: dict[str, Any], outputs: str) -> Feedback:\n",
    "    user_query = inputs[\"input\"][-1][\"content\"]\n",
    "    # Call the Judge LLM using the OpenAI SDK client.\n",
    "    judge_llm_response_obj = client.chat.completions.create(\n",
    "        model=\"databricks-meta-llama-3-3-70b-instruct\",  # This example uses Databricks hosted Claude. If you provide your own OpenAI credentials, replace with a valid OpenAI model e.g., gpt-4o-mini, etc.\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": judge_user_prompt.format(user_query=user_query, llm_response_from_app=outputs)},\n",
    "        ],\n",
    "        max_tokens=200,  # Max tokens for the judge's rationale\n",
    "        temperature=0.0, # For more deterministic judging\n",
    "    )\n",
    "    judge_llm_output_text = judge_llm_response_obj.choices[0].message.content\n",
    "    judge_output_json = json.loads(judge_llm_output_text)\n",
    "    score = judge_output_json.get(\"score\")\n",
    "    rationale = judge_output_json.get(\"rationale\")\n",
    "\n",
    "    # Parse the Judge LLM's JSON output.\n",
    "    judge_eval_json = json.loads(judge_llm_output_text)\n",
    "    parsed_score = int(judge_eval_json[\"score\"])\n",
    "    parsed_rationale = judge_eval_json[\"rationale\"]\n",
    "\n",
    "    return Feedback(\n",
    "        value=parsed_score,\n",
    "        rationale=parsed_rationale,\n",
    "        # Set the source of the assessment to indicate the LLM judge used to generate the feedback\n",
    "        source=AssessmentSource(\n",
    "            source_type=AssessmentSourceType.LLM_JUDGE,\n",
    "            source_id=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "from mlflow.genai.scorers import (\n",
    "    Guidelines,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    ")\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "with mlflow.start_run(run_name=\"Movie-Eval-LLM\"):\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=eval_df,\n",
    "        predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "        scorers=[\n",
    "            RelevanceToQuery(),\n",
    "            Safety(),\n",
    "            # You can have any number of guidelines.\n",
    "            Guidelines(\n",
    "                name=\"ReturnFormat\",\n",
    "                guidelines= \"Assess whether output contains Movie, Scene Number, Scene Description, Scene Justification\"\n",
    "            ),\n",
    "            script_fit_custom_metric\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb78d3a-f367-40fe-a9fb-7f560e1f5296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Human Evaluation\n",
    "\n",
    "Now we've got a robust LLM-as-a-Judge process, but for some problems (particularly high-value or risk, or problems that require significant contextual and subject matter understanding), human evaluation is necessary to feel confident in the solution.\n",
    "\n",
    "The **Review App** comes by default when you deploy a model. It can be used for custom labeling sessions, or just interacting directly with the agent. In this case, it is just enabled for free-form conversation with the agent.\n",
    "\n",
    "The Review App can be found at this location: https://e2-demo-west.cloud.databricks.com/ml/review-v2/chat?endpoint=agents_media_advertising-contextual_advertising-movie_scripts_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "949756c1-817d-49a0-a5ef-6c96813e76a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# END OF DEMO in E2-DEMO-WEST-WS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab1c5a9-679a-48c0-af66-b9d920e0189c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# The Code Below will not work in this Environment\n",
    "\n",
    "Due to lack of permissions. But it will work in other environments (including e2-field-demo-west). Taking this repo and leveraging in other more permissive environments will enable the functionality described below (labeling sessions for SMEs)\n",
    "\n",
    "More detail can be found at this link: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/review-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c367eb8-2523-4389-87c7-ceb449b3ccd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import mlflow\n",
    "from databricks.agents import review_app\n",
    "\n",
    "catalog_name = CATALOG\n",
    "schema_name = SCHEMA\n",
    "model_name = \"movie_scripts_chatbot_agent\" # Change to a different model name if desired\n",
    "UC_MODEL_NAME = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "uc_model_version = uc_registered_model_info.version # Update to a different version if the endpoint iterates\n",
    "user_list = [\"INSERT USERS\"] # Update with users that need permissions\n",
    "\n",
    "# Set a list of users to review the app\n",
    "# Note that <user_list> can specify individual users or groups.\n",
    "agents.set_permissions(model_name=UC_MODEL_NAME, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031200ea-06e4-4d65-a7fd-914b1b59ba24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents import datasets, review_app\n",
    "\n",
    "# register and create review app dataset\n",
    "\n",
    "exp_id = 'b822d31cbc8e4124913eabd45ad580bf' # Get experiment ID associated with deployed model\n",
    "run_id = 'a4e2e4d6b4004be390ffddfe44b6825c' # Get run ID associated with \n",
    "script_app = review_app.get_review_app(exp_id) #Experiment ID for the endpoint\n",
    "script_app = script_app.add_agent(\n",
    "  agent_name=\"script_agent\",\n",
    "  model_serving_endpoint=agent_endpoint,\n",
    "  overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2afd9d2f-dc19-4536-b84c-7c36474f3493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Labeling Sessions\n",
    "\n",
    "Below is a custom labeling session we're asking our subject matter experts to go through.\n",
    "\n",
    "`create_label_schema` defines the label details - in this case we've defined two different metrics, one is an **expectation**, which gets integrated into guidelines for the LLM as a Judge in the future, and the other as **feedback** which provides detail into the quality of the output for evaluation purposes. Our dataset contains 5 observations, and we're asking our reviewers to provide feedback on both of these metrics for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cf9438-9f97-4c69-88fa-6a90be76bbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create label schemas for the review app including instructions\n",
    "# Customize these as needed\n",
    "\n",
    "traces = mlflow.search_traces(run_id=run_id) # Need to get run_id\n",
    "quality_exp_label = script_app.create_label_schema(name='quality_expectation',\n",
    "                                        type='expectation',\n",
    "                                          title=\"Movie Quality Appropriateness for Ads\",\n",
    "                                          input=review_app.label_schemas.InputText(),\n",
    "                                          instruction=\"Evaluate whether the quality of the movie meets the requirement for the ad placement\",\n",
    "                                          enable_comment=True,\n",
    "                                          overwrite=True)\n",
    "\n",
    "quality_label = script_app.create_label_schema(name='quality_feedback',\n",
    "                                        type='feedback',\n",
    "                                          title=\"Quality of the retrieved content for Ad Placement\",\n",
    "                                          input=review_app.label_schemas.InputNumeric(min_value= 1.0, max_value=5.0),\n",
    "                                          instruction=\"\"\"Determine the quality of the retrieved content for ad placement decisions.\n",
    "                                          1=The retreived content is not relevant AND could not be used for ad placement decisions\n",
    "                                          2=The retreived content is not relevant OR could not be used for ad placement decisions\n",
    "                                          3=The retreived content is relevant and could be used for some ad placement decisions\n",
    "                                          4=The retreived content is relevant and could be used for most ad placement decisions\n",
    "                                          5=The retreived content is relevant and could be used for all ad placement decisions\"\"\",\n",
    "                                          enable_comment=True,\n",
    "                                          overwrite=True)\n",
    "\n",
    "# Create a labeling session to evaluate that specific dataset generated\n",
    "\n",
    "script_labeling_session = script_app.create_labeling_session(\n",
    "  name=\"script_demo_session\",\n",
    "  agent='script_agent',\n",
    "  assigned_users = [\"INSERT USERS\"],\n",
    "  label_schemas = ['quality_expectation', 'quality_feedback'] # Change to names of any metrics created\n",
    ")\n",
    "# Add the records from the dataset to the labeling session.\n",
    "script_labeling_session.add_dataset(uc_eval_dataset)\n",
    "# NOTE: This copies the traces into this labeling session so that labels do not modify the original traces.\n",
    "#script_labeling_session.add_traces(traces)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e5b4143-5b94-4202-810f-561b54f1d46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The dataset below is what is being updated by the user feedback. The guidelines in there include generic guideliens on each observation, as well as some feedback specific to the observation. In the second example, specific feedback not to return violent UFC or boxing movies.\n",
    "\n",
    "Now let's launch the review app and review an example, then reload the dataset to see it updated - see the link produced after running the cell above\n",
    "\n",
    "Note that this dataset is a different paradigm than the request-response approach we went through before, in this case just providing a request, and the response is generated in real time (which we will see in the UI!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30282a44-83bc-4681-90c0-49b6a28d4510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents import datasets, review_app\n",
    "\n",
    "script_app = review_app.get_review_app()\n",
    "script_labeling_session = script_app.get_labeling_sessions()[0]\n",
    "script_labeling_session.sync_expectations(uc_eval_dataset)\n",
    "display(spark.read.table(uc_eval_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc6f9914-ec0e-4358-b6fc-dde7811eb00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The cell below shows some example output generated by our reviewers in the form of feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f4f4fb-f061-4a54-a677-fb424c12dfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Check the progress of reviews\n",
    "\n",
    "for row in mlflow.search_traces(run_id=run_id).to_dict(orient='records'):\n",
    "  print(f'{row[\"request\"]}: {row[\"assessments\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6e579f-431e-439b-91d2-e51561bb5ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from IPython.display import Image\n",
    "\n",
    "image_name = \"dogfood.jpeg\" #\"catfood.jpg\"\n",
    "display(Image(image_name))\n",
    "\n",
    "w = WorkspaceClient()\n",
    "client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "with open(image_name, \"rb\") as image_file:\n",
    "  base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_data_uri = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"databricks-llama-4-maverick\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Describe this advertisement concisely in one sentence with a description of the brand and what is happening in the image\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": image_data_uri\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95764945-9586-4054-bbc9-71c83bbe9483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import SpanType\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "agent_input = f\"Where should I place the following advertisement: {response.choices[0].message.content}\"\n",
    "agent_endpoint = 'agents_media_advertising-contextual_advertising-movie_scripts_p' \n",
    "\n",
    "@mlflow.trace(span_type=SpanType.LLM)\n",
    "def invoke(ag_input):\n",
    "  agent_client = get_deploy_client()\n",
    "  span = mlflow.get_current_active_span()\n",
    "  # Set the attribute to the span\n",
    "  span.set_attributes({\"model\": agent_endpoint})\n",
    "  output = agent_client.predict(endpoint=agent_endpoint, inputs={\"messages\": [{\"role\": \"user\", \"content\": ag_input}]})\n",
    "\n",
    "  return output['choices'][0]['message']['content']\n",
    "  #payload = {\"request\": request, \"response\": output['choices'][0]['message']['content']}\n",
    "model_output = invoke(agent_input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fb7bc4-a253-4164-a8a9-ff78c077c2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(model_output)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/wesley.pasfield@databricks.com/AandE_workshop_April_2025/env2.yaml",
    "dependencies": [
     "databricks-agents>=0.16.0",
     "mlflow>=2.20.2",
     "langgraph=0.3.0",
     "unitycatalog-ai[databricks]",
     "unitycatalog-langchain[databricks]",
     "langchain_databricks"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Agent_Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
