{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86bd9db-d49c-4655-ae79-1f9ad9b092c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Movie Poster Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e59fd3a5-6af3-4922-a3db-42c8f600f9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install torch databricks-vectorsearch mlflow[databricks] transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84af3cf-1627-4ea5-be93-bd0f3e5f506e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.vectorsearch import EndpointType\n",
    "from pyspark.sql import functions as F\n",
    "import time # For polling\n",
    "import base64\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "CATALOG = \"movie_scripts\"\n",
    "SCHEMA = \"ad_placement_agent\"\n",
    "image_table = \"movie_posters\"\n",
    "embedding_model_path = f\"{CATALOG}.{SCHEMA}.image_embedding_model\"\n",
    "embedding_endpoint_name = 'clip'\n",
    "\n",
    "# Define volume path\n",
    "volume_path = \"/Volumes/movie_scripts/ad_placement_agent/movie_posters\"\n",
    "\n",
    "# Define target Delta table name for the images\n",
    "catalog_name = \"movie_scripts\"\n",
    "schema_name = \"ad_placement_agent\"\n",
    "table_name = \"movie_posters_table\"\n",
    "delta_table_full_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "# Define Vector Search endpoint and index names\n",
    "#vector_search_endpoint_name = \"movie_poster_vs_endpoint\" # User can change this name\n",
    "#vector_search_index_name = \"movie_poster_image_index\" # User can change this name\n",
    "\n",
    "# Define image embedding model endpoint name (REPLACE WITH YOUR ACTUAL IMAGE EMBEDDING MODEL ENDPOINT)\n",
    "# This should be a Databricks Model Serving endpoint that serves an image embedding model.\n",
    "#image_embedding_model_endpoint = \"my_image_embedding_model\" # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bc4241-9614-4800-8741-9232f8fc43ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Data and Store in Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf46e698-60b1-49a1-86d3-e10f4ec23f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Image Data\n",
    "print(f\"Loading image data from volume: {volume_path}\")\n",
    "# Read image files into a Spark DataFrame. The 'image' format automatically\n",
    "# parses common image file types into a struct containing image metadata and raw bytes.\n",
    "image_df = spark.read.format(\"delta\").load(volume_path).\\\n",
    "  withColumnRenamed(\"_1\", \"unique_movie_id\").\\\n",
    "  withColumnRenamed(\"_2\", 'image_binary')\n",
    "\n",
    "# Print schema to get a sense of the data\n",
    "display(image_df.limit(5))\n",
    "image_df.printSchema()\n",
    "\n",
    "# Write to a Delta table\n",
    "image_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f'{CATALOG}.{SCHEMA}.{image_table}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364b4d9b-4af0-4220-a6e9-8e9e0836cbee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Pyfunc Model\n",
    "\n",
    "This will be used for image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d02689-81e3-4f00-9f3a-7b0265e5bf2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Image embedding class\n",
    "class CLIP_IMAGE_EMBEDDING(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        # Initialize tokenizer and model\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor= CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    \n",
    "    def _get_image_embedding_bytearray(self, decoded_bytearray):\n",
    "        import base64\n",
    "        from PIL import Image\n",
    "        import requests\n",
    "        from io import BytesIO\n",
    "        image = Image.open(BytesIO(decoded_bytearray))\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features.detach().numpy().tolist()[0]\n",
    "\n",
    "    def predict(self, context, df):\n",
    "        return df['image_binary'].apply(lambda x: self._get_image_embedding_bytearray(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a153cd68-29f2-44c3-9933-f09bbff4ee44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test image Loading and Embedding Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873f00ec-ef52-4cbd-babc-9ca5ed77ad44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "#check out image\n",
    "test_image=spark.sql(f'select image_binary from {CATALOG}.{SCHEMA}.{image_table} where image_binary is not NULL limit 1').collect()[0]['image_binary']\n",
    "Image.open(BytesIO(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f755324-0ad8-4e00-ab10-a2dbd7a49418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_test_pd=spark.sql(f'select image_binary from {CATALOG}.{SCHEMA}.{image_table} where image_binary is not NULL limit 2').toPandas()\n",
    "clip=CLIP_IMAGE_EMBEDDING()\n",
    "clip.load_context(context=None)\n",
    "test_result=clip.predict(context=None, df=image_test_pd)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac5df06-b415-4840-a0f0-c2abffa6f816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Log Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f38a6de-8ab6-446d-ad32-511abd056d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.models.signature import ModelSignature, infer_signature\n",
    "\n",
    "signature1 = infer_signature(image_test_pd, [test_result[0]])\n",
    "signature2 = infer_signature(image_test_pd, [test_result[0]], params={'input_type':'text'})\n",
    "\n",
    "pip_requirements=[\n",
    "  \"--extra-index-url https://download.pytorch.org/whl/cu121\", \n",
    "  \"mlflow==2.15.1\",\n",
    "  \"setuptools<70.0.0\", \n",
    "  \"torch==2.3.1+cu121\", \n",
    "  \"accelerate==0.31.0\", \n",
    "  \"astunparse==1.6.3\", \n",
    "  \"bcrypt==3.2.0\", \n",
    "  \"boto3==1.34.39\", \n",
    "  \"configparser==5.2.0\", \n",
    "  \"defusedxml==0.7.1\", \n",
    "  \"dill==0.3.6\",\n",
    "   \"google-cloud-storage==2.10.0\", \n",
    "   \"ipython==8.15.0\", \n",
    "   \"lz4==4.3.2\", \n",
    "   \"nvidia-ml-py==12.555.43\", \n",
    "   \"optree==0.12.1\", \n",
    "   \"pandas==1.5.3\", \n",
    "   \"pyopenssl==23.2.0\", \n",
    "   \"pytesseract==0.3.10\", \n",
    "   \"scikit-learn==1.3.0\", \n",
    "   \"sentencepiece==0.1.99\", \n",
    "   \"torchvision==0.18.1+cu121\", \n",
    "   \"transformers==4.41.2\",\n",
    "   \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.3cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425cb81c-7bb3-4e72-871f-1dcfe2b1ae92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the model in Unity Catalog\n",
    "with mlflow.start_run(run_name='scripts_clip_image') as run:  \n",
    "    mlflow.pyfunc.log_model(\n",
    "        registered_model_name=embedding_model_path,\n",
    "        python_model=CLIP_IMAGE_EMBEDDING(),\n",
    "        artifact_path=\"scripts_clip_image\",\n",
    "        signature=signature1,\n",
    "        pip_requirements=pip_requirements\n",
    "    )\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "client.set_registered_model_alias(name=embedding_model_path, alias='clipimages', version=1)\n",
    "\n",
    "client.update_model_version(\n",
    "    name=embedding_model_path,\n",
    "    version=1,\n",
    "    description=\"Only does image embeddings using CLIP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4e22ec-7f88-45eb-a499-b84e233a1b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Serve the Model & Query Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd93d8de-5410-496e-b90d-b152b14873ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Define the endpoint URL and headers\n",
    "url = \"https://e2-demo-field-eng.cloud.databricks.com/api/2.0/serving-endpoints\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {notebook_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the payload for creating the model serving endpoint\n",
    "payload = {\n",
    "    \"name\": embedding_endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"entity_name\": embedding_model_path,\n",
    "                \"entity_version\": 1,\n",
    "                \"workload_size\": \"Medium\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"workload_type\": \"GPU_SMALL\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the POST request to create the serving endpoint\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Check the response status\n",
    "if response.status_code == 200:\n",
    "    print(\"Model serving endpoint created successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to create model serving endpoint: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e37dce3-e64c-419f-afed-a286a0ff225a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_test_pd=spark.sql(f'select image_binary from {CATALOG}.{SCHEMA}.{image_table} where image_binary is not NULL limit 2').toPandas()\n",
    "image_base_64=image_test_pd.head(1).iloc[0]['image_binary']\n",
    "\n",
    "\n",
    "# Define the model serving endpoint URL\n",
    "endpoint_url = f\"https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/{embedding_endpoint_name}/invocations\"\n",
    "\n",
    "input_data = {\n",
    "  \"inputs\" : [image_base_64]\n",
    "  # ,\"params\" : {'input_type':'image'} #use if using the model that can product texta nd image embeddings\n",
    "}\n",
    "\n",
    "notebook_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Set the headers for the request\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {notebook_token}'\n",
    "}\n",
    "\n",
    "# Make the request to the model serving endpoint\n",
    "response = requests.post(endpoint_url, headers=headers, data=json.dumps(input_data))\n",
    "\n",
    "# Parse the response\n",
    "response_data = response.json()\n",
    "\n",
    "# Display the response data\n",
    "display(response_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01b_Data_Preparation_Images",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
