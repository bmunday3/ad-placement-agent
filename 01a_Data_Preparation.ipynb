{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a74861d7-b2c3-4b2f-9b43-2b80c00f730a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Movie Script Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5023ceb3-6f33-431b-90e8-4a746545384b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow[databricks] langchain-text-splitters lxml transformers langchain databricks-vectorsearch bs4 markdownify torch\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3fcbea-1d95-46f1-9923-7f268026f9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"media_advertising\" \n",
    "SCHEMA = \"contextual_advertising\" \n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578170bb-3202-4cda-a9d6-340095ca7a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496811bc-a7c2-44b5-b739-10544a465ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./resources/00-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7abea6d-5ca9-4dda-93ef-72289232bb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3add086-de3d-4d65-b363-dd98b37224b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scripts_df = spark.read.format(\"delta\").load(f\"{VOLUME_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850f118d-ca2e-4091-90d1-edb04e655a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scripts_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf6be35-fcd1-483f-80da-a83aa3ae30a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scripts Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "64200ebd-dbda-43fb-b047-6ae54596e13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_count = scripts_df.count()\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7201776-a172-4965-89ce-259f4bc31d44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Print out Sample Scripts"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "num = random.randint(0, 1222)\n",
    "\n",
    "script = scripts_df.filter(scripts_df.unique_movie_id == num).select(\"script\").collect()[0][0]\n",
    "print(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff89762-d687-4cc4-9f26-0bb0d9486e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Preprocessing Pipeline\n",
    "This pipeline includes custom logic to parse the movie scripts, implement basic data cleansing, and chunking logic. The goal is to prepare our raw movie scripts in a format such that we can build a vector search endpoint on top of the Delta table. For more best practices on parsing, enrichment, and chunking strategies, visit [Build an unstructured Data Pipeline](https://docs.databricks.com/aws/en/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c223e38e-4a59-4606-8ee3-09355eaf3d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parsing & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3374e4bd-cb44-4ebe-af89-ed0583655b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, mean, stddev, min, max, median\n",
    "\n",
    "scene_header_pattern = r\"\\b(INT|EXT|INTERIOR|EXTERIOR)(\\.|\\s*\\/\\s*\\.?\\s*(EXT|INT|INTERIOR|EXTERIOR)\\.?|\\.)?(\\s|$)\"\n",
    "scene_header_regex = re.compile(scene_header_pattern, re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "@udf(\"struct<scene_count: int, scenes: array<struct<header: string, scene_number: int, text: string>>>\")\n",
    "def extract_scenes(script_text):\n",
    "    scenes = []\n",
    "    matches = list(scene_header_regex.finditer(script_text))\n",
    "    \n",
    "    # Extract text between headers\n",
    "    for i, match in enumerate(matches):\n",
    "        print(match)\n",
    "        header = match.group().strip()\n",
    "        start = match.end()  # End of the header line\n",
    "        end = matches[i+1].start() if i < len(matches)-1 else len(script_text)\n",
    "        scene_text = script_text[start:end].strip() #.replace(\"\\r\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        scene_text_cleaned = re.sub(r\"\\s+\", \" \", scene_text).strip()\n",
    "        # scene_text = script_text[start:end].replace(\"\\r\\n\", \"\").replace(\"\\t\", \"\").strip()\n",
    "        scenes.append((header, scene_text_cleaned))\n",
    "    \n",
    "    return {\"scene_count\": len(scenes), \"scenes\": [{\"header\": header, \"scene_number\": i+1, \"text\": scene_text} for i, (header, scene_text) in enumerate(scenes)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfb4b54-c92a-4e28-af9d-4712bed1dd72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scripts_with_scenes_df = scripts_df.withColumn(\"scene_count\", extract_scenes(scripts_df.script)[\"scene_count\"]) \\\n",
    "                                   .withColumn(\"scenes\", extract_scenes(scripts_df.script)[\"scenes\"])\n",
    "\n",
    "print(scripts_with_scenes_df.count())\n",
    "# filter out movies with less than 10 scenes\n",
    "scripts_filtered = scripts_with_scenes_df.filter(col(\"scene_count\") > 35)\n",
    "print(scripts_filtered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31356b22-cd04-44b4-8d2e-9a04bd2551cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scripts_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb741d0-0020-4a04-994e-5dbb9e510967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exploded_scenes_df = scripts_filtered.withColumn(\"scene\", explode(col(\"scenes\"))) \\\n",
    "                                     .select(col(\"unique_movie_id\"), \n",
    "                                             col(\"scene.scene_number\").alias(\"scene_number\"), \n",
    "                                             col(\"scene.text\").alias(\"scene_text\")) \\\n",
    "                                     .withColumn(\"unique_movie_scene_id\", concat(col(\"unique_movie_id\"), lit(\"_\"), col(\"scene_number\")))\n",
    "\n",
    "display(exploded_scenes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8504148-5fca-4cea-a52e-4a592247a7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1894ed25-3c98-42c9-bb83-ae0d1c144459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169500b1-408d-4f21-8ca9-8ddb5ffd1791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min_chunk_size=50\n",
    "max_chunk_size=500\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "\n",
    "@udf(\"array<string>\")\n",
    "def chunk_scenes(scene, min_chunk_size=50, max_chunk_size=500):\n",
    "    scene_chunks = text_splitter.split_text(scene)\n",
    "    chunks = []\n",
    "    previous_chunk = \"\"\n",
    "    # filter out very short scenes that would not provide much value if retrieved\n",
    "    for c in scene_chunks:\n",
    "      if len(tokenizer.encode(previous_chunk + c)) <= max_chunk_size/2:\n",
    "          previous_chunk += c + \"\\n\"\n",
    "      else:\n",
    "          chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "          previous_chunk = c + \"\\n\"\n",
    "    if previous_chunk:\n",
    "        chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "    return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming text_splitter and tokenizer are defined and accessible in your environment.\n",
    "# Make sure text_splitter and tokenizer are imported or defined globally\n",
    "# or passed into your function scope if they need to be initialized once per worker.\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()), PandasUDFType.SCALAR)\n",
    "def chunk_scenes_pandas_udf(scenes: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Chunks text scenes into smaller pieces using a Pandas UDF for vectorized processing.\n",
    "    Uses fixed min/max chunk sizes defined within the UDF.\n",
    "\n",
    "    Args:\n",
    "        scenes (pd.Series): A Pandas Series where each element is a scene's text (string).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Pandas Series where each element is a list of strings (the chunks for that scene).\n",
    "    \"\"\"\n",
    "    # Fixed chunk sizes directly within the UDF's scope\n",
    "    min_chunk_size = 50\n",
    "    max_chunk_size = 500\n",
    "\n",
    "    results = []\n",
    "    for scene in scenes:\n",
    "        if scene is None:\n",
    "            results.append([])\n",
    "            continue\n",
    "\n",
    "        scene_chunks = text_splitter.split_text(scene)\n",
    "        chunks = []\n",
    "        previous_chunk = \"\"\n",
    "\n",
    "        for c in scene_chunks:\n",
    "            current_chunk_stripped = c.strip()\n",
    "            if not current_chunk_stripped and not previous_chunk:\n",
    "                continue\n",
    "\n",
    "            # Ensure proper concatenation before encoding for combined length check\n",
    "            combined_text = (previous_chunk + \"\\n\" + current_chunk_stripped).strip() if previous_chunk else current_chunk_stripped\n",
    "\n",
    "            encoded_len = len(tokenizer.encode(combined_text))\n",
    "\n",
    "            if encoded_len <= max_chunk_size / 2: # Keep original logic for max_chunk_size\n",
    "                previous_chunk = combined_text # Update previous_chunk with the combined text\n",
    "            else:\n",
    "                # Process the accumulated previous_chunk\n",
    "                split_prev = text_splitter.split_text(previous_chunk.strip())\n",
    "                if split_prev:\n",
    "                    chunks.extend(split_prev)\n",
    "                # Start new previous_chunk with the current chunk\n",
    "                previous_chunk = current_chunk_stripped # Start new previous_chunk\n",
    "\n",
    "        if previous_chunk:\n",
    "            split_prev = text_splitter.split_text(previous_chunk.strip())\n",
    "            if split_prev:\n",
    "                chunks.extend(split_prev)\n",
    "\n",
    "        # Filter chunks by min_chunk_size\n",
    "        filtered_chunks = [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    "        results.append(filtered_chunks)\n",
    "\n",
    "    return pd.Series(results)\n",
    "\n",
    "@udf(\"int\")\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe513e7-3208-456c-b49d-4eed5ae57b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exploded_scenes_df_with_tokens = exploded_scenes_df.withColumn(\"token_count\", count_tokens(col(\"scene_text\")))\n",
    "exploded_scenes_df_with_tokens = exploded_scenes_df_with_tokens.filter(exploded_scenes_df_with_tokens.token_count > min_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1bb8787-e116-45b2-9719-5a373b07d811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_metadata = spark.table(\"movie_metadata\").select(\"unique_movie_id\", \"title\")\n",
    "display(movie_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead11b0c-27a2-447e-9783-28833aa07ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = exploded_scenes_df_with_tokens.join(movie_metadata, \"unique_movie_id\", \"left\")\n",
    "joined_df = joined_df.select(\"unique_movie_scene_id\", \"unique_movie_id\", \"title\", \"scene_number\", \"scene_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb5dc2-4378-4074-8646-e659cfc6c63e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(joined_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5403a8e-9619-4bd3-8051-25f30f3bc24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(joined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f435124-56a0-453f-9746-0a7a932420d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "content_table_name = f'{CATALOG}.{SCHEMA}.movie_scripts_content'\n",
    "dbutils.widgets.text(\"CONTENT_TABLE_NAME\", content_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dffa51-2289-46d2-8ce7-b07bde37202e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS IDENTIFIER('${CONTENT_TABLE_NAME}') (\n",
    "  unique_movie_scene_id STRING,\n",
    "  unique_movie_id LONG,\n",
    "  title STRING,\n",
    "  scene_number INT,\n",
    "  scene_text STRING\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "  'delta.autoOptimize.autoCompact' = 'true'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f8540b-aab8-48ff-9c75-96d12b02a577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_scripts_content = joined_df.withColumn(\"scene_text\", explode(chunk_scenes_pandas_udf(joined_df.scene_text))) \\\n",
    "  .select(col(\"unique_movie_scene_id\"),\n",
    "          col(\"unique_movie_id\"),\n",
    "          col(\"title\"),\n",
    "          col(\"scene_number\"),\n",
    "          col(\"scene_text\"))\n",
    "movie_scripts_content.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.movie_scripts_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea74ef0f-bcca-496f-8683-d7699f75d55f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_scripts_content = spark.table(\"movie_scripts_content\").select(\"unique_movie_scene_id\", \"unique_movie_id\", \"title\", \"scene_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd1e6c5-8bdb-4070-b7b6-30d7c19ef306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE movie_scripts_content\n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffffb6ac-d9ef-404b-b524-2432147c5fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE movie_scripts_content\n",
    "ALTER COLUMN unique_movie_scene_id SET NOT NULL;\n",
    "\n",
    "ALTER TABLE movie_scripts_content\n",
    "ADD CONSTRAINT pk_unique_movie_scene_id PRIMARY KEY (unique_movie_scene_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e038eb-13d0-4e90-84d2-bf227d6215f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Indexing\n",
    "In this section, we will use our cleaned Delta table to create a Vector Search Index, which will serve our script embeddings and enable the retrival component of our RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330aedde-c3a2-4fa9-8ff7-38ed5ce44574",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VS Config"
    }
   },
   "outputs": [],
   "source": [
    "# vector search config\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"one-env-shared-endpoint-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246c9814-13a5-449e-89c1-19f1862ea54a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create VS Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# create vector search endpoint if it does not already exist\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2564bd8e-2035-4bc5-8ff6-7b3bdb40658a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create VS Index"
    }
   },
   "outputs": [],
   "source": [
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{CATALOG}.{SCHEMA}.movie_scripts_content\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{CATALOG}.{SCHEMA}.movie_scripts_content_vs\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"unique_movie_scene_id\",\n",
    "    embedding_source_column='scene_text', #The column containing our text\n",
    "    embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    "  )\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94312146-a28f-4976-9199-1c4e63307035",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test VS Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "question = \"Look for a scene that targets 18-34 males for a footwear brands\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"title\", \"scene_number\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "mlflow[databricks]",
     "langchain-text-splitters==0.2.0",
     "lxml==4.9.3",
     "transformers==4.49.0",
     "langgraph==0.3.4",
     "databricks-vectorsearch==0.56",
     "bs4==0.0.2",
     "markdownify==0.14.1",
     "tiktoken==0.7.0",
     "mlflow>=2.17.2"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7178528564631620,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01a_Data_Preparation",
   "widgets": {
    "CONTENT_TABLE_NAME": {
     "currentValue": "media_advertising.contextual_advertising.movie_scripts_content",
     "nuid": "cf35670a-0eb3-412a-8a58-1d43d0309cb3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "media_advertising.contextual_advertising.movie_scripts_content",
      "label": null,
      "name": "CONTENT_TABLE_NAME",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "media_advertising.contextual_advertising.movie_scripts_content",
      "label": null,
      "name": "CONTENT_TABLE_NAME",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
