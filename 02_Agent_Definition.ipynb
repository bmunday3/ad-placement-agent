{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130ba72e-5a6f-47d0-832e-7c501f2e3310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Agent notebook\n",
    "\n",
    "This is very similar to an auto-generated notebook created by an AI Playground export. There are three notebooks in the same folder:\n",
    "- [**agent**]($./agent): contains the code to build the agent.\n",
    "\n",
    "This notebook uses Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/retrieval-augmented-generation)) to create your agent. It defines a LangChain agent that has access to tools, which we define in this notebook as well.\n",
    "\n",
    "Use this notebook to iterate on and modify the agent. For example, you could add more tools or change the system prompt.\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "After testing and iterating on your agent in this notebook, go to the auto-generated [agent-eval-deployment]($./agent-eval-deployment) notebook in this folder to log, register, evaluate, and deploy the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9931f427-d2f3-42d6-ac0a-7618119bb3dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-agents unitycatalog-ai[databricks] unitycatalog-langchain[databricks] databricks-langchain databricks-vectorsearch==0.56 langchain==0.3.20 langgraph==0.3.4 pydantic==2.11.7 mlflow[databricks]\n",
    "#langchain-community langgraph langgraph-checkpoint langchain_core==0.3.67\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59478b63-625a-4ab2-b437-a628de1ad6d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae38785-f976-4d7d-9a6e-132a4d8f2773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the chat model and tools\n",
    "Create a LangChain chat model that supports [LangGraph tool](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/) calling.\n",
    "\n",
    "We'll be importing tools from UC as well as defining a retriever. See [LangChain - How to create tools](https://python.langchain.com/v0.2/docs/how_to/custom_tools/) and [LangChain - Using built-in tools](https://python.langchain.com/v0.2/docs/how_to/tools_builtin/).\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3db30ba-aac5-4b72-b2ea-12b101357f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "llm_endpoint = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=llm_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466c6cac-c0b7-410d-a4f7-5750b29a184d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c2fade-44cc-48a0-addf-308580ff86ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from databricks_langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "\n",
    "CATALOG = 'media_advertising'\n",
    "SCHEMA = 'contextual_advertising'\n",
    "\n",
    "vs_endpoint = 'one-env-shared-endpoint-10'\n",
    "index_name = f'{CATALOG}.{SCHEMA}.movie_scripts_content_vs'\n",
    "\n",
    "# Connect to an existing Databricks Vector Search endpoint and index\n",
    "vector_store = DatabricksVectorSearch(\n",
    "  endpoint= vs_endpoint, \n",
    "  index_name= index_name, \n",
    "  columns=[\n",
    "    \"unique_movie_id\",\n",
    "    \"title\",\n",
    "    \"scene_number\",\n",
    "    \"scene_text\"\n",
    "  ]\n",
    ").as_retriever(search_kwargs={\"k\": 5}) \n",
    "#This parameter determines how many results are returned - important for retrieval tuning\n",
    "\n",
    "# Create a tool object that performs retrieval against our vector search index\n",
    "retriever_tool = create_retriever_tool(\n",
    "  vector_store,\n",
    "  name=\"search_movie_scripts\", \n",
    "  description=\"Use this tool to search for relevant script chunks from the movie scripts database and provide a recommendation of where to insert ad placements.\", \n",
    ")\n",
    "\n",
    "# Specify the return type schema of our retriever, so that evaluation and UIs can\n",
    "# automatically display retrieved chunks\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=\"unique_movie_id\",\n",
    "    text_column=\"scene_text\",\n",
    "    other_columns=[\"title\", \"scene_number\"],\n",
    "    name= index_name,\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f479273e-9ffd-44a4-94e3-e8bc272ce5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Output parsers\n",
    "Databricks interfaces, such as the AI Playground, can optionally display pretty-printed tool calls.\n",
    "\n",
    "Use the following helper functions to parse the LLM's output into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569491c0-d0d4-4929-bc15-e045fd1b45d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Any\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    MessageLikeRepresentation,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def stringify_tool_call(tool_call: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert a raw tool call into a formatted string that the playground UI expects if there is enough information in the tool_call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = json.dumps(\n",
    "            {\n",
    "                \"id\": tool_call.get(\"id\"),\n",
    "                \"name\": tool_call.get(\"name\"),\n",
    "                \"arguments\": json.dumps(tool_call.get(\"args\", {})),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "        return f\"<tool_call>{request}</tool_call>\"\n",
    "    except:\n",
    "        return str(tool_call)\n",
    "\n",
    "\n",
    "def stringify_tool_result(tool_msg: ToolMessage) -> str:\n",
    "    \"\"\"\n",
    "    Convert a ToolMessage into a formatted string that the playground UI expects if there is enough information in the ToolMessage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.dumps(\n",
    "            {\"id\": tool_msg.tool_call_id, \"content\": tool_msg.content}, indent=2\n",
    "        )\n",
    "        return f\"<tool_call_result>{result}</tool_call_result>\"\n",
    "    except:\n",
    "        return str(tool_msg)\n",
    "\n",
    "\n",
    "def parse_message(msg) -> str:\n",
    "    \"\"\"Parse different message types into their string representations\"\"\"\n",
    "    # tool call result\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return stringify_tool_result(msg)\n",
    "    # tool call\n",
    "    elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        tool_call_results = [stringify_tool_call(call) for call in msg.tool_calls]\n",
    "        return \"\".join(tool_call_results)\n",
    "    # normal HumanMessage or AIMessage (reasoning or final answer)\n",
    "    elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "        return msg.content\n",
    "    else:\n",
    "        print(f\"Unexpected message type: {type(msg)}\")\n",
    "        return str(msg)\n",
    "\n",
    "\n",
    "def wrap_output(stream: Iterator[MessageLikeRepresentation]) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Process and yield formatted outputs from the message stream.\n",
    "    The invoke and stream langchain functions produce different output formats.\n",
    "    This function handles both cases.\n",
    "    \"\"\"\n",
    "    for event in stream:\n",
    "        # the agent was called with invoke()\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                yield parse_message(msg) + \"\\n\\n\"\n",
    "        # the agent was called with stream()\n",
    "        else:\n",
    "            for node in event:\n",
    "                for key, messages in event[node].items():\n",
    "                    if isinstance(messages, list):\n",
    "                        for msg in messages:\n",
    "                            yield parse_message(msg) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        print(\"Unexpected value {messages} for key {key}. Expected a list of `MessageLikeRepresentation`'s\")\n",
    "                        yield str(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fed967-5418-4d5c-854f-f12ea01851bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the agent\n",
    "Here we provide a simple graph that uses the model and tools defined by [config.yml]($./config.yml). This graph is adapated from [this LangGraph guide](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/).\n",
    "\n",
    "\n",
    "To further customize your LangGraph agent, you can refer to:\n",
    "* [LangGraph - Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for explanations of the concepts used in this LangGraph agent\n",
    "* [LangGraph - How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/) to expand the functionality of your agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf0dc5b-2fd9-4316-b3ca-e9b6f38898a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "#from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not last_message.tool_calls:\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = SystemMessage(content=agent_prompt)\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use agent.\n",
    "        # This means these are the edges taken after the agent node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        # The mapping below will be used to determine which node to go to\n",
    "        {\n",
    "            # If tools, then we call the tool node.\n",
    "            \"continue\": \"tools\",\n",
    "            # END is a special node marking that the graph should finish.\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # We now add a unconditional edge from tools to agent.\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8948fb-2e03-4f3d-b112-59e088be5925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n",
    "\n",
    "agent_system_prompt = \"You are a RAG chatbot working for a television network company. You will be used to help make recommendations for optimal placement of advertisements based on program scripts. For this example, you have access to movie scripts, but the intention is to air commercials. Based on the user query, retrieve the most relevant scripts from the movie scenes and synthesize this information, combined with the proposed advertisement, into a helpful response that accurately recommends where to place an advertisement. Make sure the recommendation includes the title of the movie and the scene number you recommend placing the advertisement after in addition to the context and summary you provide. The scene number is a field returned by the retriever tool you have access to - it will be returned to you as 'scene_number'.\"\n",
    "\n",
    "# Create the agent with the system message if it exists\n",
    "try:\n",
    "    agent_prompt = agent_system_prompt\n",
    "    mlflow.langchain.autolog()\n",
    "    agent_with_raw_output = create_tool_calling_agent(\n",
    "        llm, \n",
    "        tools, \n",
    "        agent_prompt=agent_prompt\n",
    "    )\n",
    "except KeyError:\n",
    "    agent_with_raw_output = create_tool_calling_agent(llm, tools)\n",
    "\n",
    "agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103faa89-36ea-49f7-b48f-145996137b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since this notebook called `mlflow.langchain.autolog()` you can view the trace for each step the agent takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe32d25d-5ab3-463d-a2b6-a545e588ab3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace this placeholder input example with an appropriate domain-specific example for your agent\n",
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"When could I insert a commercial for a dog food product targeting 18-35 year old dog owners?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03180f34-eca7-4230-9517-f11cb3398a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"When could I insert a commercial for cat food?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5657bea4-8fa8-43bb-9193-c394831fbc35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log agent \n",
    "mlflow.models.set_model(agent)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/wesley.pasfield@databricks.com/AandE_workshop_April_2025/env.yaml",
    "dependencies": [
     "langchain-community",
     "langgraph==0.2.16",
     "langchain_databricks",
     "langchain",
     "langgraph-checkpoint",
     "langchain_core",
     "pydantic",
     "mlflow-skinny"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_Agent_Definition",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
